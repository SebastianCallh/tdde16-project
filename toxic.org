

  :PROPERTIES:
  :header-args: :eval never-export
  :header-args:bash: :exports code
  :header-args:elisp: :exports code
  :header-args:ipython: :exports both
  :END:

#+BEGIN_SRC bash :dir ~/.venv/ :results drawer
  pwd
  virtualenv -p python3 tdde16
#+END_SRC

#+BEGIN_SRC elisp :results silent
  (pyvenv-activate "~/.venv/tdde16")
#+END_SRC

#+BEGIN_SRC bash :results drawer :async t
  pip install ipython jupyter_client jupyter_console numpy matplotlib pandas sklearn gensim seaborn cython keras keras-metrics nltk
#+END_SRC

#+RESULTS:
: 9d8936f4403519740c778939d1b0ec03

* Text Classification Using RNN

  #+begin_src ipython  :results drawer :async  :session s :exports output
    %matplotlib inline
    from keras.preprocessing.sequence import pad_sequences
    from keras.preprocessing.text import Tokenizer
    import pandas as pd
    import numpy as np
    from typing import List 
    np.random.seed(1)

    # Load data
    train = pd.read_csv('./toxic/train.csv')
    test = pd.read_csv('./toxic/test.csv')
    truth = pd.read_csv('./toxic/test_labels.csv')
    tags = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] 
    
    max_tweet_length = 140
    EMBEDDING_DIM = 300
    MAX_N_WORDS = 20000 # n most common words to use
    MAX_SEQ_LEN = 300   # Truncate sequences to this length
    train_docs = train.comment_text.tolist()

    # There are duds in the test set, which we filter out
    test_and_truth = test.merge(truth).query('threat != -1')
    test_docs = test_and_truth.comment_text.tolist() 
  #+end_src

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

** Exploring the data
    #+BEGIN_SRC ipython :session s :async :results latex
      %matplotlib inline
      import numpy as np
      import pandas as pd

      # Load data
      train = pd.read_csv('./toxic/train.csv')
      test = pd.read_csv('./toxic/test.csv')
      truth = pd.read_csv('./toxic/test_labels.csv')
      tags = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] 

    #+END_SRC

    #+RESULTS:
    #+BEGIN_EXPORT latex
    # Out[34]:
    #+END_EXPORT

    #+BEGIN_SRC ipython :session s
      def f(o):
	  if (o.toxic == 0) \
	  and (o.severe_toxic == 0) \
	  and (o.obscene == 0) \
	  and (o.threat == 0) \
	  and (o.insult == 0) \
	  and (o.identity_hate == 0):
	    return 1
	  else: 
	    return 0

      label_df = train[tags].copy()
      label_df['ok'] = 0
      label_df.ok = label_df.apply(f, axis=1)
      label_df.head()
    #+END_SRC

      #+RESULTS:
      : # Out[18]:
      : #+BEGIN_EXAMPLE
      :   toxic  severe_toxic  obscene  threat  insult  identity_hate  ok
      :   0      0             0        0       0       0              0   1
      :   1      0             0        0       0       0              0   1
      :   2      0             0        0       0       0              0   1
      :   3      0             0        0       0       0              0   1
      :   4      0             0        0       0       0              0   1
      : #+END_EXAMPLE

      Barplot of label counts
    #+BEGIN_SRC ipython :session s :results drawer
      import matplotlib.pyplot as plt
      import matplotlib as mpl
      mpl.style.use('seaborn')
      plot_tags = ['ok'] + tags
      counts = [sum(label_df[tag]) for tag in plot_tags]
      y_pos = np.arange(len(plot_tags))
      plt.bar(y_pos, counts, align='center', alpha=0.5)
      plt.xticks(y_pos, plot_tags)
      plt.ylabel('Count')
      plt.title('Labels')
      plt.show()
      #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[37]:
    [[file:./obipy-resources/hS6TSM.png]]
    :END:

    Histogram of comment word count
    #+BEGIN_SRC ipython :session s :results drawer
      import seaborn as sns
      lengths = train.comment_text.apply(lambda x: len(x.split()))
      ax = sns.distplot(lengths)
      ax.set(xlabel='Length', ylabel='Density')
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[43]:
    : [Text(0, 0.5, 'Density'), Text(0.5, 0, 'Length')]
    [[file:./obipy-resources/mEM2xZ.png]]
    :END:


** Naive Bayes
   #+BEGIN_SRC ipython :results output :async t :session s :async
     from sklearn.naive_bayes import ComplementNB
     from sklearn.feature_extraction.text import CountVectorizer
     from sklearn.pipeline import Pipeline
     from sklearn.metrics import classification_report

     def train_naive_bayes(X, y):
	 pl = Pipeline([
	     ('vect', CountVectorizer()),
	     ('clf', ComplementNB())
	 ])
	 return pl.fit(X, y)

     nb_train_X = train.comment_text
     nb_train_y = train[tags]
     nbs = [train_naive_bayes(nb_train_X, nb_train_y[tag]) for tag in tags]

     nb_test_X = test_and_truth.comment_text
     for nb, tag in zip(nbs, tags):
	 pred = nb.predict(nb_test_X)
	 true = test_and_truth[tag]
	 print('Accuracy:', np.mean(pred == true))
	 print(classification_report(
	    true, pred, target_names=['ok', tag]))
  #+END_SRC  

#+ATTR_LATEX :caption Performance of naive Bayes models 
| Label         | Precision | Recall | F1-score |
| Severe toxic  |      0.99 |   0.95 |     0.97 |
| Obscene       |      0.95 |   0.91 |     0.92 |
| Threat        |      0.99 |   0.97 |     0.98 |
| Insult        |      0.94 |   0.91 |     0.92 |
| Identity hate |      0.98 |   0.95 |     0.97 |



|              |       Ok | Severe toxic |     OK |  Obscene |      Ok | Threat |    Ok | Insult |    Ok | Identity hate |
| Precision    |     1.00 |         0.07 |   0.98 |     0.36 |    1.00 |   0.01 |  0.98 |   0.33 |  0.99 |          0.09 |
| Recall       |     0.95 |         0.59 |   0.92 |     0.71 |    0.98 |   0.09 |  0.92 |   0.66 |  0.96 |          0.38 |
| F1-score     |     0.97 |         0.12 |   0.95 |     0.48 |    0.99 |   0.02 |  0.95 |   0.44 |  0.98 |          0.15 |
| Support      |    63611 |          367 |  60282 |     3691 |   63767 |    211 | 60551 |   3427 | 63266 |           712 |

 #+BEGIN_SRC ipython :session s
   import multiprocessing
   from gensim.models.doc2vec import Doc2Vec, TaggedDocument
   cores = multiprocessing.cpu_count()
   size = 100
   docs = [TaggedDocument(doc, [tag]) for tag, doc in enumerate(X)]
   d2v = Doc2Vec(
       docs,
       vector_size=size,
       window=10,
       min_count=2,
       workers=cores,
       hs=0,
       sample=0,
       alpha=0.05)
   d2v.train(docs, total_examples=len(docs), epochs=10)
 #+END_SRC
 

** RNNS
   This section covers the implementation of LSTM with a trained embedding layer and one using pre-trained FastText embeddings.
*** Pre-process
    We'll start off by doing some pre-processing common for both implementations. First up is sanitizing the imput and filtering it through stop words that have been augmented with some tokens found while manually inspecting the data.
   #+BEGIN_SRC ipython :session s 
     from functools import reduce
     from nltk.corpus import stopwords
     import re

     stop_words = set(stopwords.words('english'))
     stop_words.update(['==', '-', ':', '.', '"', '-', '|', '<', '>', '^', '|-',
			'/', 'i\'m', 'i\'ve', 'that\'s'])
     def sanitize(w: str) -> str:
	 stop_chars = ['"', '\'', ',', '\(', '\)', '\[', '\]', '{', '}', '<', '>', '|', '\?', '\!']
	 return reduce(lambda w, c: re.sub(c, '', w), stop_chars, w)

     def pre_process(doc: List[str]) -> List[str]:
	 return [
	     sanitize(w.lower()) for w in doc.split() 
	     if w.lower() not in stop_words
	     and len(sanitize(w.lower())) > 0
	 ]

     train_processed = [pre_process(d) for d in train_docs]
     test_processed = [pre_process(d) for d in test_docs]

     tokenizer = Tokenizer(num_words=MAX_N_WORDS)
     tokenizer.fit_on_texts(train_processed + test_processed)
     word_index = tokenizer.word_index

     train_X = pad_sequences(
	 tokenizer.texts_to_sequences(train_processed), 
	 maxlen=MAX_SEQ_LEN)
     test_X = pad_sequences(
	 tokenizer.texts_to_sequences(test_processed), 
	 maxlen=MAX_SEQ_LEN)

     train_Y = train[tags]
     test_Y = test_and_truth[tags]
     print('Shape of data tensor:', train_X.shape)
     print('Shape of label tensor:', train_Y.shape)
     print('Shape of test data tensor:', test_X.shape)
     print('Shape of test label tensor:', test_Y.shape)
   #+END_SRC

   #+RESULTS:
   : # Out[3]:

   Since filtering takes quite some time we save the data sets for later use.
   #+BEGIN_SRC ipython :session s
     np.savetxt('train-x.txt', train_X, fmt='%d')
     np.savetxt('train-y.txt', train_Y, fmt='%d')
     np.savetxt('test-x.txt', test_X, fmt='%d')
     np.savetxt('test-y. txt', test_Y, fmt='%d')
   #+END_SRC


   We also define a function for evaluating a trained model on test data.
    #+BEGIN_SRC ipython :session s 
      def eval_model(m, X, y):
	  preds = m.predict_proba(X) > .5
	  p = np.sum(preds, axis=0)
	  tp = np.sum(y, axis=0)
	  vec_max = np.vectorize(lambda x: max(x, 0))
	  fp = np.sum(vec_max(preds - y), axis=0)
	  fn = np.sum(vec_max(y - preds), axis=0)
	  recall = tp / (tp + fn)
	  precision = tp / (tp + fp)
	  print('false positives:\n', fn)
	  print('recall:\n', recall)
	  print('precision:\n', precision)
    #+END_SRC

    #+RESULTS:
    : # Out[4]:

*** Trained embeddings
    Let's now train an LSTM network and its embeddings.
    #+BEGIN_SRC ipython :session s
      from keras.models import Sequential
      from keras.layers import Dense, LSTM
      from keras.layers.embeddings import Embedding
      from keras.regularizers import l2
      import keras_metrics 
      from functools import reduce
      np.random.seed(1)

      n_hidden = 64
      n_epochs = 3
      batch_size = 128
      m = Sequential()
      m.add(Embedding(len(word_index) + 1, 
		      EMBEDDING_DIM, 
		      input_length=MAX_SEQ_LEN))
      m.add(LSTM(50))
      m.add(Dense(len(tags), activation='sigmoid'))
      m.compile(loss='binary_crossentropy',
		optimizer='adam')

      m.fit(train_X,
	    train_Y,
	    epochs=n_epochs,
	    batch_size=batch_size)
      eval_model(m, test_X, test_y)
    #+END_SRC

    #+RESULTS:
    : # Out[5]:

*** Embeddings with FastText
    Now let's do the same thing, but instead of training the embedding layer we load the weights from pre trained FastText embeddings.
   #+BEGIN_SRC ipython :session s 
     import codecs

     embeddings = {}
     with codecs.open('./toxic/wiki-news-300d-1M.vec', encoding='utf-8') as f:
       for line in f:
	 tokens = line.rstrip().rsplit(' ')
	 word = tokens[0]
	 coefs = np.asarray(tokens[1:], dtype='float32')
	 embeddings[word] = coefs
     print('loaded %s word vectors' % len(embeddings))
   #+END_SRC 

   The first layer in the network will embed words integer tokens into a vector space using FastText embeddings. The 
   embedding is made using a custom keras embedding layer, with an embedding matrix which contains the word vector 
   for word token \(i\) on row \(i\). 
   #+BEGIN_SRC ipython :session s
     EMBEDDING_DIM = 300
     embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
     n_null_embeddings = 0
     null_words = set()
     for word, i in word_index.items():
       embedding_vector = embeddings.get(word)
       if embedding_vector is not None:
	 # words not found in embedding index will be all-zeros.
	 embedding_matrix[i] = embedding_vector
       else: 
	 n_null_embeddings += 1
	 null_words.add(word)

     print(str(n_null_embeddings / len(embeddings)))
     print(n_null_embeddings, "null embeddings out of",  len(embeddings))
     print('embedding matrix dim:', embedding_matrix.shape)
     print(list(null_words)[:50])
    #+END_SRC

 #+RESULTS:
 : # Out[11]:
    Create network and train it. 
    #+BEGIN_SRC ipython :session s
      from keras.models import Sequential
      from keras.layers import Dense, LSTM
      from keras.layers.embeddings import Embedding
      from keras.regularizers import l2
      import keras_metrics 
      from functools import reduce
      np.random.seed(1)

      n_hidden = 64
      n_epochs = 3
      batch_size = 128
      m = Sequential()
      m.add(Embedding(len(word_index) + 1, 
		      EMBEDDING_DIM,
		      input_length=MAX_SEQ_LEN,
		      weights=[embedding_matrix],
		      trainable=False))
      m.add(LSTM(n_hidden)) #, input_shape=(n_hidden, 1)))
      m.add(Dense(len(tags), activation='sigmoid'))

      def all_metrics(ms, i):
	  recall = keras_metrics.recall(label=i)
	  precision = keras_metrics.precision(label=i)
	  f1_score = keras_metrics.f1_score(label=i)
	  return ms + [recall, precision, f1_score]

      metrics = reduce(all_metrics, range(len(tags)), [])

      m.compile(loss='binary_crossentropy',
		optimizer='adam',
		metrics=metrics)

      m.fit(train_X,
	    train_Y,
	    epochs=n_epochs,
	    batch_size=batch_size)
    #+END_SRC

    #+RESULTS:
    : # Out[12]:
    : : <keras.callbacks.History at 0x1de7e1f5438>

    Old stuff
    #+BEGIN_SRC ipython :session s :async t
      from gensim.models import FastText
      vec_size = 200
      ft_file = "./toxic/fasttext.bin"
      ft_vocab = test_docs + train_docs
      ft = FastText(ft_vocab, size=vec_size, window=6, min_count=1, iter=10)
      ft.save(ft_file)
      #ft = FastText.load(ft_file)
      #if w in ft.wv
      #	 else np.empty(vec_size) 
      embedding_matrix = np.vstack([
	  ft.wv[w] 
	  for w in word_index.keys()
      ])

      n_null_embeddings = sum(np.all(embedding_matrix, axis=1))
      n_null_embeddings
     #+END_SRC

    #+BEGIN_SRC ipython :session s :async t
      from gensim.models import FastText

      def tokenize(doc: List[str]) -> List[str]:
	  return [w.lower() for w in doc.split() 
		  if w.isalpha()
		  and len(w) >= min_len]

      min_len = 2
      docs = train.comment_text
      #vocab = docs.apply(tokenize)
      #vocab = vocab[vocab.transform(lambda x: len(x) > min_len)]
      vec_size = 200
      ft_file = "./toxic/fasttext.bin"
      #ft = FastText(vocab, size=vec_size, window=6, min_count=min_len, iter=10)
      ft.save(ft_file)
      ft = FastText.load(ft_file)
      # Word embed stuff

      def mean_vector_embed(ft, tokens: List[str]) -> List[int]:
	  return np.mean([ft[t] if t in ft else np.empty(vec_size) for t in tokens], axis=0)

      här ska vi bygga embedding matrix och köra ett embeddinglager med den 
      tags = ['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
      tokens = docs.apply(tokenize)
      mask = tokens.transform(lambda x: len(x) > min_len)
      train_Y = train[tags][mask]
      train_X = np.array([mean_vector_embed(ft , t) for t in tokens[mask]])
    #+END_SRC

    #+RESULTS:
    : 0 - a7f71db2-3515-43bf-b980-f01d99ae52f1



** Classification with LSTM Network
    #+BEGIN_SRC ipython :session s :async
    def train_lstm(train_X, train_Y, n_hidden, n_epochs, batch_size):
	m = Sequential()
	m.add(Embedding(n_words, n_hidden, 
			input_length=max_tweet_length))
	m.add(LSTM(50))
	m.add(Dense(len(tags), activation='sigmoid'))

	def all_metrics(ms, i):
	    recall = keras_metrics.recall(label=0)
	    precision = keras_metrics.precision(label=0)
	    f1_score = keras_metrics.f1_score(label=0)
	    return ms + [recall, precision, f1_score]

	metrics = reduce(all_metrics, range(len(tags)), [])
	print(metrics)

	m.compile(loss='binary_crossentropy', 
		  optimizer='adam', 
		  metrics=['accuracy'] + metrics)

	m.fit(train_X, train_Y , epochs=n_epochs, batch_size=batch_size)
	scores = m.evaluate(test_X, test_Y, verbose=0)
	scores
    #+END_SRC

*** Using FastText embeddings
    #+BEGIN_SRC ipython :session s :async
      from gensim.models import FastText
      docs = train.comment_text
      vocab = train.comment_text.apply(tokenize)
      vocab = vocab[vocab.transform(lambda x: len(x) > min_len)]
      ft_file = "./toxic/fasttext.bin"
      #ft = FastText(vocab, size=200, window=6, min_count=min_len, iter=10)
      #ft.save(fname)
      #ft = FastText.load(ft_file)
      # Word embed stuff
      def mean_vector_embed(docs, labels):
	  token_lists = docs.apply(tokenize)
	  thing = pd.concat([token_lists, labels])
	  non_empty_token_lists = token_lists[token_lists.transform(lambda x: len(x) > min_len)]
	  print(non_empty_token_lists)
	  word_vectors = [np.mean([ft[t] for t in ts 
				   if t in ft], axis=0)
			  for ts in non_empty_token_lists]
	  return word_vectors

      #ft_train_X = np.array(mean_vector_embed(docs))
      tags = ['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
      ft_train_Y = train[tags]
      n_hidden = 50
      data_dim = ft_train_X.shape[1]
      m = Sequential()
      m.add(Dense(data_dim))
      m.add(LSTM(n_hidden))
      m.add(Dense(len(tags), activation='sigmoid'))

      def all_metrics(ms, i):
	  recall = keras_metrics.recall(label=0)
	  precision = keras_metrics.precision(label=0)
	  f1_score = keras_metrics.f1_score(label=0)
	  return ms + [recall, precision, f1_score]

      metrics = reduce(all_metrics, range(len(tags)), [])
      print(metrics)

      m.compile(loss='binary_crossentropy', 
		optimizer='adam', 
		metrics=['accuracy'] + metrics)

      m.fit(ft_train_X, ft_train_Y , epochs=n_epochs, batch_size=batch_size)
      scores = m.evaluate(test_X, test_Y, verbose=0)
      scores
    #+END_SRC
#+BEGIN_SRC ipython :session s
ft_train_Y.shape
#+END_SRC

#+RESULTS:
: # Out[62]:
: : (159571, 5)

*** Using one-hot encodings
     #+BEGIN_SRC ipython :session s :output none
       from gensim.models import FastText
       from keras.models import Sequential
       from keras.layers import Dense, LSTM
       from keras.layers.embeddings import Embedding
       from keras.regularizers import l2
       import keras_metrics 
       from functools import reduce
       np.random.seed(1)

       # Integer representation of training data
       train_X = sequence.pad_sequences(
	   sequences=tokenizer.texts_to_sequences(train_docs), 
	   maxlen=max_tweet_length)
       train_Y = train[tags]

       # Integer representation of test data
       # There are duds in the test set, which we filter out
       test_and_truth = test.merge(truth).query('threat != -1')
       test_docs = list(test_and_truth.comment_text.values)
       test_X = sequence.pad_sequences(
	   sequences=tokenizer.texts_to_sequences(test_docs),
	   maxlen=max_tweet_length)
       test_Y = test_and_truth[tags]

       tags = ['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] 
       #ft = FastText.load("./toxic/fasttext.bin")
       n_hidden = 64
       n_epochs = 3
       batch_size = 128
       m = Sequential()
       m.add(Embedding(n_words, n_hidden, 
		       input_length=max_tweet_length))
       m.add(LSTM(50))
       m.add(Dense(len(tags), activation='sigmoid'))

       def all_metrics(ms, i):
	      recall = keras_metrics.recall(label=0)
	      precision = keras_metrics.precision(label=0)
	      f1_score = keras_metrics.f1_score(label=0)
	      return ms + [recall, precision, f1_score]

       metrics = reduce(all_metrics, range(len(tags)), [])
       print(metrics)

       m.compile(loss='binary_crossentropy', 
		 optimizer='adam', 
		 metrics=['accuracy'] + metrics)

       m.fit(train_X, train_Y , epochs=n_epochs, batch_size=batch_size)
       scores = m.evaluate(test_X, test_Y, verbose=0)
       scores
     print("Loss: %.2f" % scores[0])
     print("Accuracy: %.2f" % scores[1])
  #+END_SRC


