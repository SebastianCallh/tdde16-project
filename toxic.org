[[http://text-analytics101.rxnlp.com/2014/10/all-about-stop-words-for-text-mining.html][stoplist]]

  :PROPERTIES:
  :header-args: :eval never-export
  :header-args:bash: :exports code
  :header-args:elisp: :exports code
  :header-args:ipython: :exports both
  :END:

#+BEGIN_SRC bash :dir ~/.venv/ :results drawer
  pwd
  virtualenv -p python3 tdde16
#+END_SRC

#+BEGIN_SRC elisp :results silent
  (pyvenv-activate "~/.venv/tdde16")
#+END_SRC

#+BEGIN_SRC bash :results drawer :async t
  pip install ipython jupyter_client jupyter_console numpy matplotlib pandas sklearn gensim seaborn cython keras
#+END_SRC

* Text Classification Using RNN

** Loading the data
   #+begin_src ipython  :results drawer :async t :session s :exports output
     %matplotlib inline
     from keras.preprocessing import sequence
     from keras.preprocessing.text import Tokenizer
     import pandas as pd
     import numpy as np
     np.random.seed(1)

     # Load data
     train = pd.read_csv('./toxic/train.csv')
     test = pd.read_csv('./toxic/test.csv')
     truth = pd.read_csv('./toxic/test_labels.csv')
     tags = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] 

     # Create tokenizer
     max_tweet_length = 140
     n_words = 5000 # n most common words
     train_docs = list(train.comment_text.values)
     tokenizer = Tokenizer(num_words=n_words)
     tokenizer.fit_on_texts(train_docs)

     # Integer representation of training data
     train_X = sequence.pad_sequences(
	 sequences=tokenizer.texts_to_sequences(train_docs), 
	 maxlen=max_tweet_length)
     train_Y = train[tags]

     # Integer representation of test data
     # There are duds in the test set, which we filter out
     test_and_truth = test.merge(truth).query('threat != -1')
     test_docs = list(test_and_truth.comment_text.values)
     test_X = sequence.pad_sequences(
	 sequences=tokenizer.texts_to_sequences(test_docs),
	 maxlen=max_tweet_length)
     test_Y = test_and_truth[tags]
   #+end_src

   #+RESULTS:
   :RESULTS:
   0 - 6c92c39a-0a37-42de-9d0a-fcd0d146e1ff
   :END:

** Exploring the data
    #+BEGIN_SRC ipython :session s :async :results latex
      %matplotlib inline
      import numpy as np
      import pandas as pd

      # Load data
      train = pd.read_csv('./toxic/train.csv')
      test = pd.read_csv('./toxic/test.csv')
      truth = pd.read_csv('./toxic/test_labels.csv')
      tags = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] 

    #+END_SRC

    #+RESULTS:
    #+BEGIN_EXPORT latex
    # Out[34]:
    #+END_EXPORT

    #+BEGIN_SRC ipython :session s
      def f(o):
	  if (o.toxic == 0) \
	  and (o.severe_toxic == 0) \
	  and (o.obscene == 0) \
	  and (o.threat == 0) \
	  and (o.insult == 0) \
	  and (o.identity_hate == 0):
	    return 1
	  else: 
	    return 0

      label_df = train[tags].copy()
      label_df['ok'] = 0
      label_df.ok = label_df.apply(f, axis=1)
      label_df.head()
    #+END_SRC

      #+RESULTS:
      : # Out[18]:
      : #+BEGIN_EXAMPLE
      :   toxic  severe_toxic  obscene  threat  insult  identity_hate  ok
      :   0      0             0        0       0       0              0   1
      :   1      0             0        0       0       0              0   1
      :   2      0             0        0       0       0              0   1
      :   3      0             0        0       0       0              0   1
      :   4      0             0        0       0       0              0   1
      : #+END_EXAMPLE

      Barplot of label counts
    #+BEGIN_SRC ipython :session s :results drawer
      import matplotlib.pyplot as plt
      import matplotlib as mpl
      mpl.style.use('seaborn')
      plot_tags = ['ok'] + tags
      counts = [sum(label_df[tag]) for tag in plot_tags]
      y_pos = np.arange(len(plot_tags))
      plt.bar(y_pos, counts, align='center', alpha=0.5)
      plt.xticks(y_pos, plot_tags)
      plt.ylabel('Count')
      plt.title('Labels')
      plt.show()
      #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[37]:
    [[file:./obipy-resources/hS6TSM.png]]
    :END:

    Histogram of comment word count
    #+BEGIN_SRC ipython :session s :results drawer
      import seaborn as sns
      lengths = train.comment_text.apply(lambda x: len(x.split()))
      ax = sns.distplot(lengths)
      ax.set(xlabel='Length', ylabel='Density')
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[43]:
    : [Text(0, 0.5, 'Density'), Text(0.5, 0, 'Length')]
    [[file:./obipy-resources/mEM2xZ.png]]
    :END:

** Naive Bayes
   #+BEGIN_SRC ipython :results output :async t :session s :async
     from sklearn.naive_bayes import ComplementNB
     from sklearn.feature_extraction.text import CountVectorizer
     from sklearn.pipeline import Pipeline
     from sklearn.metrics import classification_report

     def train_naive_bayes(X, y):
	 pl = Pipeline([
	     ('vect', CountVectorizer()),
	     ('clf', ComplementNB())
	 ])
	 return pl.fit(X, y)

     nb_train_X = train.comment_text
     nb_train_y = train[tags]
     nbs = [train_naive_bayes(nb_train_X, nb_train_y[tag]) for tag in tags]

     nb_test_X = test_and_truth.comment_text
     for nb, tag in zip(nbs, tags):
	 pred = nb.predict(nb_test_X)
	 true = test_and_truth[tag]
	 print('Accuracy:', np.mean(pred == true))
	 print(classification_report(
	    true, pred, target_names=['ok', tag]))
  #+END_SRC  

  #+RESULTS:
  #+begin_example
  Accuracy: 0.9498890243521211
		precision    recall  f1-score   support

	    ok       1.00      0.95      0.97     63611
  severe_toxic       0.07      0.59      0.12       367

     micro avg       0.95      0.95      0.95     63978
     macro avg       0.53      0.77      0.55     63978
  weighted avg       0.99      0.95      0.97     63978

  Accuracy: 0.909468879927475
		precision    recall  f1-score   support

	    ok       0.98      0.92      0.95     60287
       obscene       0.36      0.71      0.48      3691

     micro avg       0.91      0.91      0.91     63978
     macro avg       0.67      0.82      0.71     63978
  weighted avg       0.95      0.91      0.92     63978

  Accuracy: 0.9722873487761418
		precision    recall  f1-score   support

	    ok       1.00      0.98      0.99     63767
	threat       0.01      0.09      0.02       211

     micro avg       0.97      0.97      0.97     63978
     macro avg       0.50      0.53      0.50     63978
  weighted avg       0.99      0.97      0.98     63978

  Accuracy: 0.9085466879239739
		precision    recall  f1-score   support

	    ok       0.98      0.92      0.95     60551
	insult       0.33      0.66      0.44      3427

     micro avg       0.91      0.91      0.91     63978
     macro avg       0.65      0.79      0.69     63978
  weighted avg       0.94      0.91      0.92     63978

  Accuracy: 0.9528587952108537
		 precision    recall  f1-score   support

	     ok       0.99      0.96      0.98     63266
  identity_hate       0.09      0.38      0.15       712

      micro avg       0.95      0.95      0.95     63978
      macro avg       0.54      0.67      0.56     63978
   weighted avg       0.98      0.95      0.97     63978

  #+end_example

 #+BEGIN_SRC ipython :session s
   import multiprocessing
   from gensim.models.doc2vec import Doc2Vec, TaggedDocument
   cores = multiprocessing.cpu_count()
   size = 100
   docs = [TaggedDocument(doc, [tag]) for tag, doc in enumerate(X)]
   d2v = Doc2Vec(
       docs,
       vector_size=size,
       window=10,
       min_count=2,
       workers=cores,
       hs=0,
       sample=0,
       alpha=0.05)
   d2v.train(docs, total_examples=len(docs), epochs=10)
 #+END_SRC
 
** Embeddings with FastText
   #+BEGIN_SRC ipython :session s
     import re
     min_len = 2
     def f(w): 
	 w = w.lower()
	 #w = re.sub(r'^https?:\/\/.*[\r\n]*', '', w, flags=re.MULTILINE)
	 return w

     def tokenize(phrase):	 
	 return [f(w) for w in phrase.split() 
		 if w.isalpha()
		 and len(w) >= min_len]

     for x in train[train.comment_text.str.contains('qf')].comment_text:
	 print(tokenize(x))
   #+END_SRC

   #+RESULTS:
   : # Out[68]:

   #+BEGIN_SRC ipython :session s
     from gensim.models import FastText
     vocabulary = train_X.apply(tokenize)
     vocabulary = vocabulary[vocabulary.transform(lambda x: len(x) > min_len)]
     ft = FastText(vocabulary, size=200, window=6, min_count=min_len, iter=10)
     fname = "./toxic/fasttext.bin"
     ft.save(fname)

     # Word embed stuff
     def mean_vector_embed(phrases):
	 token_lists = phrases.apply(tokenize)
	 non_empty_token_lists = token_lists[token_lists.transform(lambda x: len(x) > min_len)]
	 for x in non_empty_token_lists:
	     if x not in ft.wv:
		 print(x)

	 print(non_empty_token_lists)
	 word_vectors = [np.mean(ft[ts], axis=1) for ts in non_empty_token_lists]
	 return word_vectors

     input_vectors = mean_vector_embed(train_X)
   #+END_SRC

   #+RESULTS:
   : # Out[56]:

** Classification with LSTM Network
   #+BEGIN_SRC ipython :session s :async
     from gensim.models import FastText
     from keras.models import Sequential
     from keras.layers import Dense, LSTM
     from keras.layers.embeddings import Embedding

     tags = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] 
     #ft = FastText.load("./toxic/fasttext.bin")
     n_hidden = 64
     n_epochs = 3
     batch_size = 128
     m = Sequential()
     m.add(Embedding(n_words, n_hidden, input_length=max_tweet_length))
     m.add(LSTM(50))
     m.add(Dense(len(tags), activation='sigmoid'))
     m.compile(loss='binary_crossentropy', 
	       optimizer='adam', 
	       metrics=['accuracy'])

     m.fit(train_X, train_Y , epochs=n_epochs, batch_size=batch_size)
     scores = m.evaluate(test_X, test_Y, verbose=0)
     print("Loss: %.2f" % scores[0])
     print("Accuracy: %.2f" % scores[1])
  #+END_SRC
  
