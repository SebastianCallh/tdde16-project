\chapter{Method}
This chapter describes the data pre-processing and the implementation
process. The implementation goal was to create functional
implementation of an LSTM network using pre-trained FastText
embeddings~\cite{joulin2016bag,joulin2016fasttext}, and a Naive Bayes classifier.

\section{Pre-processing}
The first step was to sanitise the comments using regex
substitution. Stop words from the Python NLTK~\cite{nltk-stopwords}
was used, together with custom rewrites removing parentheses, brackets, hyphens,
and performing additional rewrites to get the corpus on a format
compatible with the pre-trained embeddings. An example of these
rewrites were removing apostrophes, such that the word
\textit{shouldn't} mapped to \textit{shouldnt}.

\section{Naive Bayes Baseline model}
Building on the assumption that some key words are enough to identify the
category of a document, a Naive Bayes model was used as a baseline
model. Scikit-learn~\cite{scikit-learn} was used for the
implementation, both for the tokenisation and the classifier.

\section{LSTM networks}\label{sec:lstm}
It reasonable to believe that a sequence of words carry more
information than a single word without context, and that a model using
sequences would perform better than a model that does not. For this,
two slightly different LSTM networks were used, implemented using the
Keras framework~\cite{chollet2015keras}. The difference of the two
networks was the embeddings they used. The first network used an
embedding layer which learned the embeddings as part of the network
training process, and the second used pre-trained FastText embeddings.

\subsection{With learned embeddings}
The first network had one embedding layer, one LSTM layer with 50 hidden
units, and one output layer with the sigmoid activation
function. Hyper-parameters for the implementation
were picked manually base on inspecting the data. Maximum sequence length was set
to 200 (all sequences shorter were zero padded to this length), only
the 15000 most common words were tokenised, and the embedding
dimension was 300. The network was trained for 3 epochs.

\subsection{With FastText embeddings} 
Because of frequent misspellings in online conversations,
it is reasonable to believe that words that are very similar could
mean the same thing. Since FastText use the substructure of words when
making embeddings, it was hypothesised that it could capture
some misspellings and improve performance. A network with the same
structure and parameters described previously, but with a FastText embedding layer was
consequently implemented and evaluated. This was done by manually
assigning the weights of the embedding layer and freezing it during training~\cite{keras-using-embeddings}.
47\% of words were not found in the pre-trained embeddings, and were simply
embedded as the null vector.